{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f312e9",
   "metadata": {},
   "source": [
    "# Using Different Models with Autogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a004f6",
   "metadata": {},
   "source": [
    "## OpenAI - Already seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f0145",
   "metadata": {},
   "source": [
    "# Ollama - Local hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3713dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' usage=RequestUsage(prompt_tokens=33, completion_tokens=23) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"Who are you, who created you?\", source=\"user\")])\n",
    "print(response)\n",
    "await ollama_model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786aee3",
   "metadata": {},
   "source": [
    "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c9a3d",
   "metadata": {},
   "source": [
    "finish_reason='stop' content=\"I am a computer program designed to simulate conversation and answer questions to the best of my ability. I'm a type of artificial intelligence (AI) called a large language model, which means I've been trained on a massive dataset of text from various sources.\\n\\nMy primary function is to assist users with information and tasks, and I do this by processing natural language inputs and generating human-like responses. I don't have personal experiences, emotions, or consciousness like humans do, but I'm designed to be helpful and informative.\\n\\nI was created by a team of researcher-engineers at Meta AI, which is a subsidiary of Meta Platforms, Inc. The development of my technology involved extensive research in natural language processing (NLP), machine learning, and cognitive architectures.\\n\\nMy training data consists of a massive corpus of text from various sources, including books, articles, research papers, and websites. This corpus was used to fine-tune my understanding of language patterns, semantics, and context.\\n\\nI'm constantly learning and improving through interactions with users like you, so I appreciate any feedback or corrections that can help me become a better conversational AI!\" usage=RequestUsage(prompt_tokens=33, completion_tokens=226) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='I am an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' usage=RequestUsage(prompt_tokens=33, completion_tokens=23) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"Who are you, who created you?\", source=\"user\")])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee8a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "agent_1 = AssistantAgent(name='MyOllamaAgent',model_client=ollama_model_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent_1. run(task='Tell me something about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60cccc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='a6b436b1-ca2a-430f-bf42-cb46e2ac0185', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 6, 598002, tzinfo=datetime.timezone.utc), content='Tell me something about you?', type='TextMessage'), TextMessage(id='ad293544-9fd3-4610-9826-536bf9855c2c', source='MyOllamaAgent', models_usage=RequestUsage(prompt_tokens=55, completion_tokens=136), metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 16, 631317, tzinfo=datetime.timezone.utc), content=\"I'm an artificial intelligence designed to assist and communicate with users like you. I have been trained on a vast amount of text data, which enables me to understand and respond to a wide range of questions and topics.\\n\\nMy primary function is to provide helpful and accurate information, answer questions, and engage in conversations to the best of my abilities. I can process natural language inputs and generate human-like responses.\\n\\nI don't have personal experiences, emotions, or opinions like humans do, but I'm designed to be neutral and informative. My goal is to help users find the information they need, learn new things, or simply have a conversation.\\n\\nWhat would you like to talk about?\", type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468544ed",
   "metadata": {},
   "source": [
    "TaskResult(messages=[TextMessage(id='a6b436b1-ca2a-430f-bf42-cb46e2ac0185', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 6, 598002, tzinfo=datetime.timezone.utc), content='Tell me something about you?', type='TextMessage'), TextMessage(id='ad293544-9fd3-4610-9826-536bf9855c2c', source='MyOllamaAgent', models_usage=RequestUsage(prompt_tokens=55, completion_tokens=136), metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 16, 631317, tzinfo=datetime.timezone.utc), content=\"I'm an artificial intelligence designed to assist and communicate with users like you. I have been trained on a vast amount of text data, which enables me to understand and respond to a wide range of questions and topics.\\n\\nMy primary function is to provide helpful and accurate information, answer questions, and engage in conversations to the best of my abilities. I can process natural language inputs and generate human-like responses.\\n\\nI don't have personal experiences, emotions, or opinions like humans do, but I'm designed to be neutral and informative. My goal is to help users find the information they need, learn new things, or simply have a conversation.\\n\\nWhat would you like to talk about?\", type='TextMessage')], stop_reason=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbb9cb",
   "metadata": {},
   "source": [
    "# GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476a5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615f0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='Canberra\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=3) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=GOOGLE_API_KEY,\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of australia?\", source=\"user\")])\n",
    "print(response)\n",
    "await model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932f356",
   "metadata": {},
   "source": [
    "finish_reason='stop' content='Paris\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=2) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19328eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75178e85",
   "metadata": {},
   "source": [
    "# Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67dfcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_api_key = os.getenv('open_router_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87265c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "model_info is required when model name is not a valid OpenAI model",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m open_router_model_client =  \u001b[43mOpenAIChatCompletionClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://openrouter.ai/api/v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenrouter/cypher-alpha:free\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_router_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m open_router_model_client.create([UserMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen/autogen-venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:1474\u001b[39m, in \u001b[36mOpenAIChatCompletionClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1471\u001b[39m client = _openai_client_from_config(copied_args)\n\u001b[32m   1472\u001b[39m create_args = _create_args_from_config(copied_args)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_capabilities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_capabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_name_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_name_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_name_in_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_name_in_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1481\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen/autogen-venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:435\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.__init__\u001b[39m\u001b[34m(self, client, create_args, model_capabilities, model_info, add_name_prefixes, include_name_in_message)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_capabilities \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m         \u001b[38;5;28mself\u001b[39m._model_info = \u001b[43m_model_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_args\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodel_info is required when model name is not a valid OpenAI model\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen/autogen-venv/lib/python3.12/site-packages/autogen_ext/models/openai/_model_info.py:521\u001b[39m, in \u001b[36mget_info\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    510\u001b[39m model_info: ModelInfo = _MODEL_INFO.get(\n\u001b[32m    511\u001b[39m     resolved_model,\n\u001b[32m    512\u001b[39m     {\n\u001b[32m   (...)\u001b[39m\u001b[32m    518\u001b[39m     },\n\u001b[32m    519\u001b[39m )\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_info.get(\u001b[33m\"\u001b[39m\u001b[33mfamily\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mFAILED\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodel_info is required when model name is not a valid OpenAI model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_info.get(\u001b[33m\"\u001b[39m\u001b[33mfamily\u001b[39m\u001b[33m\"\u001b[39m) == ModelFamily.UNKNOWN:\n\u001b[32m    523\u001b[39m     trace_logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel info not found for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: model_info is required when model name is not a valid OpenAI model"
     ]
    }
   ],
   "source": [
    "\n",
    "open_router_model_client =  OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"openrouter/cypher-alpha:free\",\n",
    "    api_key = open_router_api_key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\" :True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "response = await open_router_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902354c",
   "metadata": {},
   "source": [
    "finish_reason='stop' content='The capital of France is **Paris**. Known for iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral, Paris has long served as the political, cultural, and economic heart of France. It houses key government institutions, including the official residence of the French President (Élysée Palace) and the National Assembly.' usage=RequestUsage(prompt_tokens=13, completion_tokens=409) cached=False logprobs=None thought=None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
